{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf7ccb32",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain-academy/blob/main/module-2/chatbot-external-memory.ipynb) [![Open in LangChain Academy](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66e9eba12c7b7688aa3dbb5e_LCA-badge-green.svg)](https://academy.langchain.com/courses/take/intro-to-langgraph/lessons/58239440-lesson-6-chatbot-w-summarizing-messages-and-external-memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6c7afe-1037-41ab-98e4-494692e47402",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# å…·å¤‡æ¶ˆæ¯æ€»ç»“ä¸å¤–éƒ¨æ•°æ®åº“è®°å¿†çš„èŠå¤©æœºå™¨äºº\n",
    "\n",
    "## å›é¡¾\n",
    "\n",
    "æˆ‘ä»¬å·²ç»è®²è¿‡å¦‚ä½•è‡ªå®šä¹‰å›¾çš„çŠ¶æ€ï¼ˆstateï¼‰æ¨¡å¼ä»¥åŠ reducerï¼ˆå½’çº¦å™¨ï¼‰ã€‚\n",
    "\n",
    "ä¹Ÿå±•ç¤ºäº†å¤šç§åœ¨å›¾çŠ¶æ€ä¸­è£å‰ªæˆ–è¿‡æ»¤æ¶ˆæ¯çš„æŠ€å·§ã€‚\n",
    "\n",
    "æˆ‘ä»¬ç”¨è¿™äº›æ¦‚å¿µå®ç°è¿‡ä¸€ä¸ªå¸¦è®°å¿†çš„èŠå¤©æœºå™¨äººï¼Œå®ƒä¼šç”Ÿæˆå¯¹è¯çš„æ»šåŠ¨æ‘˜è¦ã€‚\n",
    "\n",
    "## ç›®æ ‡\n",
    "\n",
    "ä½†æ˜¯ï¼Œå¦‚æœæˆ‘ä»¬å¸Œæœ›èŠå¤©æœºå™¨äººæ‹¥æœ‰â€œå¯ä»¥æ— é™æœŸæŒä¹…åŒ–â€çš„è®°å¿†æ€ä¹ˆåŠï¼Ÿ\n",
    "\n",
    "ç°åœ¨ï¼Œæˆ‘ä»¬å°†å¼•å…¥ä¸€äº›æ›´é«˜çº§çš„æ£€æŸ¥ç‚¹ï¼ˆcheckpointerï¼‰ï¼Œå®ƒä»¬æ”¯æŒå¤–éƒ¨æ•°æ®åº“ã€‚\n",
    "\n",
    "è¿™é‡Œæˆ‘ä»¬å±•ç¤ºå¦‚ä½•ä½¿ç”¨ [Sqlite ä½œä¸º checkpointer](https://docs.langchain.com/oss/python/langgraph/persistence#checkpointer-libraries)ï¼Œå½“ç„¶è¿˜å¯ä»¥é€‰æ‹© Postgres ç­‰å…¶ä»–å®ç°ï¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ed78d9-6ca2-45ac-96a9-52e341ec519d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install --quiet -U langgraph-checkpoint-sqlite langchain_core langgraph langchain_openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e10c4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from rich import print as rprint\n",
    "\n",
    "# è·å–é¡¹ç›®æ ¹ç›®å½•ï¼ˆå½“å‰æ–‡ä»¶æ‰€åœ¨ç›®å½•çš„ä¸Šä¸€çº§ï¼‰\n",
    "project_root = Path(__file__).parent.parent if '__file__' in globals() else Path.cwd().parent\n",
    "env_path = project_root / '.env'\n",
    "\n",
    "# åŠ è½½ .env æ–‡ä»¶\n",
    "load_dotenv(dotenv_path=env_path)\n",
    "\n",
    "required_vars = ['OPENAI_API_KEY', 'OPENAI_API_BASE', 'MODEL_NAME', \"LANGSMITH_API_KEY\", \"LANGSMITH_TRACING\", \"LANGSMITH_PROJECT\"]\n",
    "missing_vars = [var for var in required_vars if not os.environ.get(var)]\n",
    "\n",
    "if missing_vars:\n",
    "    rprint(f\"[red]è­¦å‘Š: ä»¥ä¸‹ç¯å¢ƒå˜é‡æœªåœ¨ .env æ–‡ä»¶ä¸­é…ç½®: {', '.join(missing_vars)}[/red]\")\n",
    "else:\n",
    "    rprint(f\"[green]âœ“ ç¯å¢ƒå˜é‡å·²ä» {env_path} æˆåŠŸåŠ è½½[/green]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40d25c0-e9b5-4854-bf07-3cc3ff07122e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Sqlite\n",
    "\n",
    "ä¸€ä¸ªå¥½çš„èµ·æ­¥æ–¹å¼æ˜¯ä½¿ç”¨ [SqliteSaver checkpointer](https://docs.langchain.com/oss/python/langgraph/persistence#checkpointer-libraries)ã€‚\n",
    "\n",
    "Sqlite æ˜¯ä¸€ä¸ª[å°å·§ã€å¿«é€Ÿã€éå¸¸æµè¡Œ](https://x.com/karpathy/status/1819490455664685297)çš„ SQL æ•°æ®åº“ã€‚\n",
    "\n",
    "å¦‚æœæˆ‘ä»¬æä¾› `\":memory:\"`ï¼Œå®ƒä¼šåˆ›å»ºä¸€ä¸ªå†…å­˜ä¸­çš„ Sqlite æ•°æ®åº“ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae15402-17ae-4e89-8ecf-4c89e08b22fe",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "# In memory\n",
    "conn = sqlite3.connect(\":memory:\", check_same_thread = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2bf53ec-6d4a-42ce-8183-344795eed403",
   "metadata": {},
   "source": [
    "ä½†æ˜¯ï¼Œå¦‚æœæˆ‘ä»¬æä¾›ä¸€ä¸ªæ•°æ®åº“è·¯å¾„ï¼Œå®ƒå°±ä¼šä¸ºæˆ‘ä»¬åˆ›å»ºä¸€ä¸ªçœŸæ­£çš„æ•°æ®åº“æ–‡ä»¶ï¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58339167-920c-4994-a0a7-0a9c5d4f7cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull file if it doesn't exist and connect to local db\n",
    "!mkdir -p state_db && [ ! -f state_db/example.db ] && wget -P state_db https://github.com/langchain-ai/langchain-academy/raw/main/module-2/state_db/example.db\n",
    "\n",
    "db_path = \"state_db/example.db\"\n",
    "conn = sqlite3.connect(db_path, check_same_thread=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7736b6-a750-48f8-a838-8e7616b12250",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Here is our checkpointer \n",
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "memory = SqliteSaver(conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8cb629-213f-4b87-965e-19b812c42da1",
   "metadata": {},
   "source": [
    "æˆ‘ä»¬æ¥é‡æ–°å®šä¹‰ä¸€ä¸‹èŠå¤©æœºå™¨äººã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc414e29-2078-41a0-887c-af1a6a3d72c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import Literal\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, RemoveMessage\n",
    "\n",
    "from langgraph.graph import END\n",
    "from langgraph.graph import MessagesState\n",
    "\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o\",temperature=0)\n",
    "\n",
    "class State(MessagesState):\n",
    "    summary: str\n",
    "\n",
    "# Define the logic to call the model\n",
    "def call_model(state: State):\n",
    "    \n",
    "    # Get summary if it exists\n",
    "    summary = state.get(\"summary\", \"\")\n",
    "\n",
    "    # If there is summary, then we add it\n",
    "    if summary:\n",
    "        \n",
    "        # Add summary to system message\n",
    "        system_message = f\"Summary of conversation earlier: {summary}\"\n",
    "\n",
    "        # Append summary to any newer messages\n",
    "        messages = [SystemMessage(content=system_message)] + state[\"messages\"]\n",
    "    \n",
    "    else:\n",
    "        messages = state[\"messages\"]\n",
    "    \n",
    "    response = model.invoke(messages)\n",
    "    return {\"messages\": response}\n",
    "\n",
    "def summarize_conversation(state: State):\n",
    "    \n",
    "    # First, we get any existing summary\n",
    "    summary = state.get(\"summary\", \"\")\n",
    "\n",
    "    # Create our summarization prompt \n",
    "    if summary:\n",
    "        \n",
    "        # A summary already exists\n",
    "        summary_message = (\n",
    "            f\"This is summary of the conversation to date: {summary}\\n\\n\"\n",
    "            \"Extend the summary by taking into account the new messages above:\"\n",
    "        )\n",
    "        \n",
    "    else:\n",
    "        summary_message = \"Create a summary of the conversation above:\"\n",
    "\n",
    "    # Add prompt to our history\n",
    "    messages = state[\"messages\"] + [HumanMessage(content=summary_message)]\n",
    "    response = model.invoke(messages)\n",
    "    \n",
    "    # Delete all but the 2 most recent messages\n",
    "    delete_messages = [RemoveMessage(id=m.id) for m in state[\"messages\"][:-2]]\n",
    "    return {\"summary\": response.content, \"messages\": delete_messages}\n",
    "\n",
    "# Determine whether to end or summarize the conversation\n",
    "def should_continue(state: State)-> Literal [\"summarize_conversation\",END]:\n",
    "    \n",
    "    \"\"\"Return the next node to execute.\"\"\"\n",
    "    \n",
    "    messages = state[\"messages\"]\n",
    "    \n",
    "    # If there are more than six messages, then we summarize the conversation\n",
    "    if len(messages) > 6:\n",
    "        return \"summarize_conversation\"\n",
    "    \n",
    "    # Otherwise we can just end\n",
    "    return END"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c13c0b-a383-4f73-9cc1-63f0eed8f190",
   "metadata": {},
   "source": [
    "ç°åœ¨ï¼Œæˆ‘ä»¬åªéœ€è¦ç”¨åˆšæ‰çš„ sqlite checkpointer é‡æ–°ç¼–è¯‘å³å¯ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e867fd95-91eb-4ce1-82fc-bb72d611a96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "from langgraph.graph import StateGraph, START\n",
    "\n",
    "# Define a new graph\n",
    "workflow = StateGraph(State)\n",
    "workflow.add_node(\"conversation\", call_model)\n",
    "workflow.add_node(summarize_conversation)\n",
    "\n",
    "# Set the entrypoint as conversation\n",
    "workflow.add_edge(START, \"conversation\")\n",
    "workflow.add_conditional_edges(\"conversation\", should_continue)\n",
    "workflow.add_edge(\"summarize_conversation\", END)\n",
    "\n",
    "# Compile\n",
    "graph = workflow.compile(checkpointer=memory)\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8769db99-3938-45e6-a594-56beb18d6c45",
   "metadata": {},
   "source": [
    "ç°åœ¨ï¼Œæˆ‘ä»¬å¯ä»¥å¤šæ¬¡è°ƒç”¨è¿™ä¸ªå›¾ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4094a0-d240-4be8-903a-7d9f605bdc5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a thread\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "# Start conversation\n",
    "input_message = HumanMessage(content=\"hi! I'm Lance\")\n",
    "output = graph.invoke({\"messages\": [input_message]}, config) \n",
    "for m in output['messages'][-1:]:\n",
    "    m.pretty_print()\n",
    "\n",
    "input_message = HumanMessage(content=\"what's my name?\")\n",
    "output = graph.invoke({\"messages\": [input_message]}, config) \n",
    "for m in output['messages'][-1:]:\n",
    "    m.pretty_print()\n",
    "\n",
    "input_message = HumanMessage(content=\"i like the 49ers!\")\n",
    "output = graph.invoke({\"messages\": [input_message]}, config) \n",
    "for m in output['messages'][-1:]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f3e842-4497-45e2-a924-69672a9bcb33",
   "metadata": {},
   "source": [
    "æˆ‘ä»¬æ¥ç¡®è®¤ä¸€ä¸‹çŠ¶æ€ç¡®å®è¢«ä¿å­˜åœ¨æœ¬åœ°ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ab158a-5a82-417a-8841-730a4cc18ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "graph_state = graph.get_state(config)\n",
    "graph_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e21152d-ed9c-408d-b7d5-f634c9ce81e2",
   "metadata": {},
   "source": [
    "### çŠ¶æ€æŒä¹…åŒ–\n",
    "\n",
    "ä½¿ç”¨ Sqlite è¿™æ ·çš„æ•°æ®åº“æ„å‘³ç€çŠ¶æ€æ˜¯æŒä¹…åŒ–çš„ï¼\n",
    "\n",
    "ä¾‹å¦‚ï¼šæˆ‘ä»¬å¯ä»¥é‡å¯ notebook çš„å†…æ ¸ï¼Œç„¶åä»ç„¶å¯ä»¥ä»ç£ç›˜ä¸Šçš„ Sqlite æ•°æ®åº“åŠ è½½ä¹‹å‰çš„çŠ¶æ€ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a44dc5-be04-45fa-a6fc-27b0f8ee4678",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a thread\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "graph_state = graph.get_state(config)\n",
    "graph_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8466e418-1a46-4cdb-a51a-6ae14281bb85",
   "metadata": {},
   "source": [
    "## Studio\n",
    "\n",
    "**âš ï¸ æ³¨æ„**\n",
    "\n",
    "è‡ªå½•åˆ¶è¿™äº›è§†é¢‘ä»¥æ¥ï¼Œæˆ‘ä»¬å·²æ›´æ–° Studioï¼Œä½¿å…¶ç°åœ¨å¯ä»¥åœ¨æœ¬åœ°è¿è¡Œå¹¶é€šè¿‡æµè§ˆå™¨è®¿é—®ã€‚è¿™æ˜¯æ›¿ä»£è§†é¢‘ä¸­æ‰€ç¤ºæ¡Œé¢åº”ç”¨ï¼ˆDesktop Appï¼‰çš„é¦–é€‰è¿è¡Œæ–¹å¼ã€‚å®ƒç°åœ¨è¢«ç§°ä¸º _LangSmith Studio_ï¼Œè€Œä¸æ˜¯ä¹‹å‰çš„ _LangGraph Studio_ã€‚è¯¦ç»†çš„è®¾ç½®è¯´æ˜åœ¨è¯¾ç¨‹å¼€å¤´çš„ â€œGetting Setupâ€ æŒ‡å—ä¸­ã€‚ä½ å¯ä»¥åœ¨[è¿™é‡Œ](https://docs.langchain.com/langsmith/studio)æ‰¾åˆ° Studio çš„æè¿°ï¼Œæœ¬åœ°éƒ¨ç½²çš„å…·ä½“ç»†èŠ‚åœ¨[è¿™é‡Œ](https://docs.langchain.com/langsmith/quick-start-studio#local-development-server)ã€‚  \n",
    "è¦å¯åŠ¨æœ¬åœ°å¼€å‘æœåŠ¡å™¨ï¼Œè¯·åœ¨æœ¬æ¨¡å—çš„ `/studio` ç›®å½•ä¸­è¿è¡Œï¼š\n",
    "\n",
    "```\n",
    "langgraph dev\n",
    "```\n",
    "\n",
    "ä½ åº”å½“çœ‹åˆ°å¦‚ä¸‹è¾“å‡ºï¼š\n",
    "```\n",
    "- ğŸš€ API: http://127.0.0.1:2024\n",
    "- ğŸ¨ Studio UI: https://smith.langchain.com/studio/?baseUrl=http://127.0.0.1:2024\n",
    "- ğŸ“š API Docs: http://127.0.0.1:2024/docs\n",
    "```\n",
    "\n",
    "åœ¨æµè§ˆå™¨ä¸­æ‰“å¼€ä¸Šé¢æ˜¾ç¤ºçš„ **Studio UI** URLã€‚\n",
    "åŠ è½½ Studio ä¸­çš„ `chatbot`ï¼Œå®ƒä½¿ç”¨ `module-2/studio/chatbot.py`ï¼Œå¹¶åœ¨ `module-2/studio/langgraph.json` ä¸­è¿›è¡Œäº†é…ç½®ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4916d8b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41d4491-612e-4698-aa60-de76277422bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
