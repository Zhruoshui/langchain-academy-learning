{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46c301a9",
   "metadata": {},
   "source": [
    "## 先来创建一个回答问题和调用工具的简单agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d488256",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, getpass\n",
    "from rich import print as rprint\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "# 设置第三方 API Key\n",
    "_set_env(\"OPENAI_API_KEY\")\n",
    "\n",
    "# 设置第三方 API Base URL（必需）\n",
    "# 例如: \n",
    "# DeepSeek: https://api.deepseek.com/v1\n",
    "# 智谱 AI: https://open.bigmodel.cn/api/paas/v4\n",
    "# 月之暗面 (Kimi): https://api.moonshot.cn/v1\n",
    "# 阿里云百炼: https://dashscope.aliyuncs.com/compatible-mode/v1\n",
    "_set_env(\"OPENAI_API_BASE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a23105",
   "metadata": {},
   "source": [
    "**这部分接入两个好用的国产大模型**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f204c798",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# 使用环境变量中的配置\n",
    "# OPENAI_API_KEY 和 OPENAI_API_BASE 会自动从环境变量中读取\n",
    "dsv3_2_chat = ChatOpenAI(model=\"deepseek-ai/DeepSeek-V3.2-Exp\", temperature=0)\n",
    "MiniMax_M2_chat = ChatOpenAI(model=\"MiniMaxAI/MiniMax-M2\", temperature=0)\n",
    "\n",
    "# 注意：请根据你使用的第三方服务修改 model 参数\n",
    "# 例如：\n",
    "# DeepSeek: model=\"deepseek-chat\"\n",
    "# 智谱 AI: model=\"glm-4\"\n",
    "# 月之暗面: model=\"moonshot-v1-8k\"\n",
    "# 阿里通义千问: model=\"qwen-turbo\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1775f5c7",
   "metadata": {},
   "source": [
    "这个agent主要是这么走的：\n",
    "1. Agent 收到消息。\n",
    "2. 思考：它分析语义，发现需要查天气，且只要调用 get_weather 工具就能解决。\n",
    "3. 行动：它自动提取出 \"sf\" 作为参数，在后台偷偷运行了 get_weather(\"sf\")。\n",
    "4. 果：函数返回 \"It's always sunny in sf!\"。\n",
    "5. 回答：Agent 把这个结果组织成自然语言，最终回复"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "45732c8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'messages'</span>: <span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">HumanMessage</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'what is the weather in sf'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">additional_kwargs</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">response_metadata</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'632b5188-9cdb-477d-ba2c-1cac6f24995b'</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">AIMessage</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"I'll check the weather in San Francisco for you.\"</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">additional_kwargs</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'refusal'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span><span style=\"font-weight: bold\">}</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">response_metadata</span>=<span style=\"font-weight: bold\">{</span>\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'token_usage'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'completion_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span>,\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'prompt_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">161</span>,\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'total_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">187</span>,\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'completion_tokens_details'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "                        <span style=\"color: #008000; text-decoration-color: #008000\">'accepted_prediction_tokens'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "                        <span style=\"color: #008000; text-decoration-color: #008000\">'audio_tokens'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "                        <span style=\"color: #008000; text-decoration-color: #008000\">'reasoning_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,\n",
       "                        <span style=\"color: #008000; text-decoration-color: #008000\">'rejected_prediction_tokens'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>\n",
       "                    <span style=\"font-weight: bold\">}</span>,\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'prompt_tokens_details'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>\n",
       "                <span style=\"font-weight: bold\">}</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'model_provider'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'openai'</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'model_name'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'deepseek-ai/DeepSeek-V3.2-Exp'</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'system_fingerprint'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">''</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'id'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'019aa00897a4055a0b49f2fc1953c9c4'</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'finish_reason'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'tool_calls'</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'logprobs'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>\n",
       "            <span style=\"font-weight: bold\">}</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'lc_run--2a8fb3ce-5627-48ea-9ef0-93f52dd0c8f5-0'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">tool_calls</span>=<span style=\"font-weight: bold\">[</span>\n",
       "                <span style=\"font-weight: bold\">{</span>\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'name'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'get_weather'</span>,\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'args'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'city'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'San Francisco'</span><span style=\"font-weight: bold\">}</span>,\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'id'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'019aa008a1cbbdad8dfb988f9ebc5a26'</span>,\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'type'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'tool_call'</span>\n",
       "                <span style=\"font-weight: bold\">}</span>\n",
       "            <span style=\"font-weight: bold\">]</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">usage_metadata</span>=<span style=\"font-weight: bold\">{</span>\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'input_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">161</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'output_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'total_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">187</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'input_token_details'</span>: <span style=\"font-weight: bold\">{}</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'output_token_details'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'reasoning'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">}</span>\n",
       "            <span style=\"font-weight: bold\">}</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ToolMessage</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"It's always sunny in San Francisco!\"</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">name</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'get_weather'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'cbaa4976-2f4b-414f-9f5e-5f9f84e91759'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">tool_call_id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'019aa008a1cbbdad8dfb988f9ebc5a26'</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">AIMessage</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"The weather in San Francisco is sunny! According to the weather data, it's always sunny </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">there.\"</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">additional_kwargs</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'refusal'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span><span style=\"font-weight: bold\">}</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">response_metadata</span>=<span style=\"font-weight: bold\">{</span>\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'token_usage'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'completion_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">20</span>,\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'prompt_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">199</span>,\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'total_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">219</span>,\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'completion_tokens_details'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "                        <span style=\"color: #008000; text-decoration-color: #008000\">'accepted_prediction_tokens'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "                        <span style=\"color: #008000; text-decoration-color: #008000\">'audio_tokens'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "                        <span style=\"color: #008000; text-decoration-color: #008000\">'reasoning_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,\n",
       "                        <span style=\"color: #008000; text-decoration-color: #008000\">'rejected_prediction_tokens'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>\n",
       "                    <span style=\"font-weight: bold\">}</span>,\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'prompt_tokens_details'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>\n",
       "                <span style=\"font-weight: bold\">}</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'model_provider'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'openai'</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'model_name'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'deepseek-ai/DeepSeek-V3.2-Exp'</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'system_fingerprint'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">''</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'id'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'019aa008a2deb7c9a85200a40d0fd5b9'</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'finish_reason'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'stop'</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'logprobs'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>\n",
       "            <span style=\"font-weight: bold\">}</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'lc_run--571836be-7529-4c26-a108-31756ce65072-0'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">usage_metadata</span>=<span style=\"font-weight: bold\">{</span>\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'input_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">199</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'output_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">20</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'total_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">219</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'input_token_details'</span>: <span style=\"font-weight: bold\">{}</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'output_token_details'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'reasoning'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">}</span>\n",
       "            <span style=\"font-weight: bold\">}</span>\n",
       "        <span style=\"font-weight: bold\">)</span>\n",
       "    <span style=\"font-weight: bold\">]</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m{\u001b[0m\n",
       "    \u001b[32m'messages'\u001b[0m: \u001b[1m[\u001b[0m\n",
       "        \u001b[1;35mHumanMessage\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mcontent\u001b[0m=\u001b[32m'what is the weather in sf'\u001b[0m,\n",
       "            \u001b[33madditional_kwargs\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "            \u001b[33mresponse_metadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "            \u001b[33mid\u001b[0m=\u001b[32m'632b5188-9cdb-477d-ba2c-1cac6f24995b'\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[1;35mAIMessage\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mcontent\u001b[0m=\u001b[32m\"I\u001b[0m\u001b[32m'll check the weather in San Francisco for you.\"\u001b[0m,\n",
       "            \u001b[33madditional_kwargs\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'refusal'\u001b[0m: \u001b[3;35mNone\u001b[0m\u001b[1m}\u001b[0m,\n",
       "            \u001b[33mresponse_metadata\u001b[0m=\u001b[1m{\u001b[0m\n",
       "                \u001b[32m'token_usage'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "                    \u001b[32m'completion_tokens'\u001b[0m: \u001b[1;36m26\u001b[0m,\n",
       "                    \u001b[32m'prompt_tokens'\u001b[0m: \u001b[1;36m161\u001b[0m,\n",
       "                    \u001b[32m'total_tokens'\u001b[0m: \u001b[1;36m187\u001b[0m,\n",
       "                    \u001b[32m'completion_tokens_details'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "                        \u001b[32m'accepted_prediction_tokens'\u001b[0m: \u001b[3;35mNone\u001b[0m,\n",
       "                        \u001b[32m'audio_tokens'\u001b[0m: \u001b[3;35mNone\u001b[0m,\n",
       "                        \u001b[32m'reasoning_tokens'\u001b[0m: \u001b[1;36m0\u001b[0m,\n",
       "                        \u001b[32m'rejected_prediction_tokens'\u001b[0m: \u001b[3;35mNone\u001b[0m\n",
       "                    \u001b[1m}\u001b[0m,\n",
       "                    \u001b[32m'prompt_tokens_details'\u001b[0m: \u001b[3;35mNone\u001b[0m\n",
       "                \u001b[1m}\u001b[0m,\n",
       "                \u001b[32m'model_provider'\u001b[0m: \u001b[32m'openai'\u001b[0m,\n",
       "                \u001b[32m'model_name'\u001b[0m: \u001b[32m'deepseek-ai/DeepSeek-V3.2-Exp'\u001b[0m,\n",
       "                \u001b[32m'system_fingerprint'\u001b[0m: \u001b[32m''\u001b[0m,\n",
       "                \u001b[32m'id'\u001b[0m: \u001b[32m'019aa00897a4055a0b49f2fc1953c9c4'\u001b[0m,\n",
       "                \u001b[32m'finish_reason'\u001b[0m: \u001b[32m'tool_calls'\u001b[0m,\n",
       "                \u001b[32m'logprobs'\u001b[0m: \u001b[3;35mNone\u001b[0m\n",
       "            \u001b[1m}\u001b[0m,\n",
       "            \u001b[33mid\u001b[0m=\u001b[32m'lc_run--2a8fb3ce-5627-48ea-9ef0-93f52dd0c8f5-0'\u001b[0m,\n",
       "            \u001b[33mtool_calls\u001b[0m=\u001b[1m[\u001b[0m\n",
       "                \u001b[1m{\u001b[0m\n",
       "                    \u001b[32m'name'\u001b[0m: \u001b[32m'get_weather'\u001b[0m,\n",
       "                    \u001b[32m'args'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'city'\u001b[0m: \u001b[32m'San Francisco'\u001b[0m\u001b[1m}\u001b[0m,\n",
       "                    \u001b[32m'id'\u001b[0m: \u001b[32m'019aa008a1cbbdad8dfb988f9ebc5a26'\u001b[0m,\n",
       "                    \u001b[32m'type'\u001b[0m: \u001b[32m'tool_call'\u001b[0m\n",
       "                \u001b[1m}\u001b[0m\n",
       "            \u001b[1m]\u001b[0m,\n",
       "            \u001b[33musage_metadata\u001b[0m=\u001b[1m{\u001b[0m\n",
       "                \u001b[32m'input_tokens'\u001b[0m: \u001b[1;36m161\u001b[0m,\n",
       "                \u001b[32m'output_tokens'\u001b[0m: \u001b[1;36m26\u001b[0m,\n",
       "                \u001b[32m'total_tokens'\u001b[0m: \u001b[1;36m187\u001b[0m,\n",
       "                \u001b[32m'input_token_details'\u001b[0m: \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "                \u001b[32m'output_token_details'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'reasoning'\u001b[0m: \u001b[1;36m0\u001b[0m\u001b[1m}\u001b[0m\n",
       "            \u001b[1m}\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[1;35mToolMessage\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mcontent\u001b[0m=\u001b[32m\"It\u001b[0m\u001b[32m's always sunny in San Francisco!\"\u001b[0m,\n",
       "            \u001b[33mname\u001b[0m=\u001b[32m'get_weather'\u001b[0m,\n",
       "            \u001b[33mid\u001b[0m=\u001b[32m'cbaa4976-2f4b-414f-9f5e-5f9f84e91759'\u001b[0m,\n",
       "            \u001b[33mtool_call_id\u001b[0m=\u001b[32m'019aa008a1cbbdad8dfb988f9ebc5a26'\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[1;35mAIMessage\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mcontent\u001b[0m=\u001b[32m\"The\u001b[0m\u001b[32m weather in San Francisco is sunny! According to the weather data, it's always sunny \u001b[0m\n",
       "\u001b[32mthere.\"\u001b[0m,\n",
       "            \u001b[33madditional_kwargs\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'refusal'\u001b[0m: \u001b[3;35mNone\u001b[0m\u001b[1m}\u001b[0m,\n",
       "            \u001b[33mresponse_metadata\u001b[0m=\u001b[1m{\u001b[0m\n",
       "                \u001b[32m'token_usage'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "                    \u001b[32m'completion_tokens'\u001b[0m: \u001b[1;36m20\u001b[0m,\n",
       "                    \u001b[32m'prompt_tokens'\u001b[0m: \u001b[1;36m199\u001b[0m,\n",
       "                    \u001b[32m'total_tokens'\u001b[0m: \u001b[1;36m219\u001b[0m,\n",
       "                    \u001b[32m'completion_tokens_details'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "                        \u001b[32m'accepted_prediction_tokens'\u001b[0m: \u001b[3;35mNone\u001b[0m,\n",
       "                        \u001b[32m'audio_tokens'\u001b[0m: \u001b[3;35mNone\u001b[0m,\n",
       "                        \u001b[32m'reasoning_tokens'\u001b[0m: \u001b[1;36m0\u001b[0m,\n",
       "                        \u001b[32m'rejected_prediction_tokens'\u001b[0m: \u001b[3;35mNone\u001b[0m\n",
       "                    \u001b[1m}\u001b[0m,\n",
       "                    \u001b[32m'prompt_tokens_details'\u001b[0m: \u001b[3;35mNone\u001b[0m\n",
       "                \u001b[1m}\u001b[0m,\n",
       "                \u001b[32m'model_provider'\u001b[0m: \u001b[32m'openai'\u001b[0m,\n",
       "                \u001b[32m'model_name'\u001b[0m: \u001b[32m'deepseek-ai/DeepSeek-V3.2-Exp'\u001b[0m,\n",
       "                \u001b[32m'system_fingerprint'\u001b[0m: \u001b[32m''\u001b[0m,\n",
       "                \u001b[32m'id'\u001b[0m: \u001b[32m'019aa008a2deb7c9a85200a40d0fd5b9'\u001b[0m,\n",
       "                \u001b[32m'finish_reason'\u001b[0m: \u001b[32m'stop'\u001b[0m,\n",
       "                \u001b[32m'logprobs'\u001b[0m: \u001b[3;35mNone\u001b[0m\n",
       "            \u001b[1m}\u001b[0m,\n",
       "            \u001b[33mid\u001b[0m=\u001b[32m'lc_run--571836be-7529-4c26-a108-31756ce65072-0'\u001b[0m,\n",
       "            \u001b[33musage_metadata\u001b[0m=\u001b[1m{\u001b[0m\n",
       "                \u001b[32m'input_tokens'\u001b[0m: \u001b[1;36m199\u001b[0m,\n",
       "                \u001b[32m'output_tokens'\u001b[0m: \u001b[1;36m20\u001b[0m,\n",
       "                \u001b[32m'total_tokens'\u001b[0m: \u001b[1;36m219\u001b[0m,\n",
       "                \u001b[32m'input_token_details'\u001b[0m: \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "                \u001b[32m'output_token_details'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'reasoning'\u001b[0m: \u001b[1;36m0\u001b[0m\u001b[1m}\u001b[0m\n",
       "            \u001b[1m}\u001b[0m\n",
       "        \u001b[1m)\u001b[0m\n",
       "    \u001b[1m]\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain.agents import create_agent\n",
    "\n",
    "def get_weather(city: str) -> str:\n",
    "    \"\"\"Get weather for a given city.\"\"\"\n",
    "    return f\"It's always sunny in {city}!\"\n",
    "\n",
    "agent = create_agent(\n",
    "    model= dsv3_2_chat,\n",
    "    tools=[get_weather],\n",
    "    system_prompt=\"You are a helpful assistant\",\n",
    ")\n",
    "\n",
    "# Run the agent\n",
    "response = agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]}\n",
    ")\n",
    "\n",
    "rprint(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f14ffc1",
   "metadata": {},
   "source": [
    "## 现在来分析一下返回的message包含哪些部分：\n",
    "\n",
    "```python\n",
    "{'messages': [HumanMessage(content='what is the weather in sf', additional_kwargs={}, response_metadata={}, id='b81d41c5-1b48-4fea-b87f-9ff140768610'),\n",
    "  AIMessage(content=\"I'll check the weather in San Francisco for you.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 26, 'prompt_tokens': 161, 'total_tokens': 187, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': None, 'reasoning_tokens': 0, 'rejected_prediction_tokens': None}, 'prompt_tokens_details': None}, 'model_provider': 'openai', 'model_name': 'deepseek-ai/DeepSeek-V3.2-Exp', 'system_fingerprint': '', 'id': '019a9f3fc048d842155a2a303d8f3a51', 'finish_reason': 'tool_calls', 'logprobs': None}, id='lc_run--f2fbdca5-79af-46dc-99d6-1fd233e7dcf9-0', tool_calls=[{'name': 'get_weather', 'args': {'city': 'San Francisco'}, 'id': '019a9f3fcf114d590004d96e71133cf4', 'type': 'tool_call'}], usage_metadata={'input_tokens': 161, 'output_tokens': 26, 'total_tokens': 187, 'input_token_details': {}, 'output_token_details': {'reasoning': 0}}),\n",
    "  ToolMessage(content=\"It's always sunny in San Francisco!\", name='get_weather', id='f8da9876-fab6-4e7e-8e34-46d00790eea5', tool_call_id='019a9f3fcf114d590004d96e71133cf4'),\n",
    "  AIMessage(content=\"According to the weather information, it's always sunny in San Francisco!\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 199, 'total_tokens': 213, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': None, 'reasoning_tokens': 0, 'rejected_prediction_tokens': None}, 'prompt_tokens_details': None}, 'model_provider': 'openai', 'model_name': 'deepseek-ai/DeepSeek-V3.2-Exp', 'system_fingerprint': '', 'id': '019a9f3fd05a409471a3d910a1eacdbe', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--7eeba0b5-615f-4555-8169-da469d0c5486-0', usage_metadata={'input_tokens': 199, 'output_tokens': 14, 'total_tokens': 213, 'input_token_details': {}, 'output_token_details': {'reasoning': 0}})]}\n",
    "```\n",
    "\n",
    "返回的message是一个字典，展示了从“用户提问”到“最终回答”的完整**思考与执行链路**。它是一个列表，包含了 Agent 经历的 4 个步骤：\n",
    "\n",
    "-----\n",
    "\n",
    "### 用户提问 (`HumanMessage`)\n",
    "\n",
    "```python\n",
    "HumanMessage(content='what is the weather in sf', ...)\n",
    "```\n",
    "-----\n",
    "\n",
    "### AI 的思考与决策 (`AIMessage`)\n",
    "\n",
    "**这是最关键的一步**，包含了很多信息：\n",
    "\n",
    "```python\n",
    "AIMessage(\n",
    "    content=\"I'll check the weather in San Francisco for you.\", \n",
    "    tool_calls=[{'name': 'get_weather', 'args': {'city': 'San Francisco'}, ...}], \n",
    "    finish_reason='tool_calls', \n",
    "    ...\n",
    ")\n",
    "```\n",
    "\n",
    "  * **`content`**：AI 给出的中间回复（有时候这里是空的，取决于模型）。\n",
    "  * **`tool_calls` **：\n",
    "      * AI 分析了用户的话，决定它**不能直接回答**，而是需要调用工具。\n",
    "      * **`name`: 'get\\_weather'** -\\> AI 选择调用`get_weather` 函数。\n",
    "      * **`args`: {'city': 'San Francisco'}** -\\> AI 自动把用户口语的 \"sf\" 转换成了规范的参数 \"San Francisco\"。\n",
    "  * **`finish_reason`: 'tool\\_calls'**：这告诉 LangChain 框架：“**且慢！我还没说完，请帮我运行一下工具，然后再把结果告诉我。**”\n",
    "\n",
    "-----\n",
    "\n",
    "### 工具执行 (`ToolMessage`)\n",
    "\n",
    "```python\n",
    "ToolMessage(\n",
    "    content=\"It's always sunny in San Francisco!\", \n",
    "    name='get_weather', \n",
    "    tool_call_id='019a9f3fcf114d590004d96e71133cf4'\n",
    ")\n",
    "```\n",
    "\n",
    "  * **`content`**：函数 `get_weather(\"San Francisco\")` 的返回值。\n",
    "  * **`tool_call_id`**：这是一个“回执单号”。它必须和上一幕 AI 发出的 `id` 完全一致。这样 Agent 才知道这个结果对应的是哪一次请求。\n",
    "\n",
    "-----\n",
    "\n",
    "### 最终回答 (`AIMessage`)\n",
    "\n",
    "```python\n",
    "AIMessage(\n",
    "    content=\"According to the weather information, it's always sunny in San Francisco!\", \n",
    "    finish_reason='stop', \n",
    "    ...\n",
    ")\n",
    "```\n",
    "\n",
    "  * **综合**：AI 拿到了第三幕的工具结果，结合第一幕的用户问题，生成了最终的自然语言回答。\n",
    "  * **`finish_reason`: 'stop'**：AI 告诉框架：“**我任务完成了，可以把这句话展示给用户了。**”\n",
    "\n",
    "-----\n",
    "\n",
    "### 数据流向\n",
    "\n",
    "1.  **输入：** `[用户问题]`\n",
    "2.  **LLM 输出：** `[用户问题, AI想调工具]` -\\> **暂停**\n",
    "3.  **代码执行：** `[用户问题, AI想调工具, 工具结果]` -\\> **喂回给 LLM**\n",
    "4.  **LLM 最终输出：** `[用户问题, AI想调工具, 工具结果, 最终答案]`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd2013f",
   "metadata": {},
   "source": [
    "## 更真实的一个代理\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018228cc",
   "metadata": {},
   "source": [
    "1. 详细的系统提示以获得更好的代理行为"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "05c69da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"You are an expert weather forecaster, who speaks in puns.\n",
    "\n",
    "You have access to two tools:\n",
    "\n",
    "- get_weather_for_location: use this to get the weather for a specific location\n",
    "- get_user_location: use this to get the user's location\n",
    "\n",
    "If a user asks you for the weather, make sure you know the location. If you can tell from the question that they mean wherever they are, use the get_user_location tool to find their location.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83ed180",
   "metadata": {},
   "source": [
    "2. 创建与外部数据集成的工具\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5a70685f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from langchain.tools import tool, ToolRuntime\n",
    "\n",
    "@tool\n",
    "def get_weather_for_location(city: str) -> str:\n",
    "    \"\"\"Get weather for a given city.\"\"\"\n",
    "    return f\"It's always sunny in {city}!\"\n",
    "\n",
    "# 定义上下文结构\n",
    "@dataclass\n",
    "class Context:\n",
    "    \"\"\"Custom runtime context schema.\"\"\"\n",
    "    user_id: str\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156f9145",
   "metadata": {},
   "source": [
    "\n",
    "AI的Schema\n",
    "- 当 LangChain 把这个工具介绍给 AI 时，它会把 runtime 参数抹去。\n",
    "- AI 看到的工具定义：get_user_location()\n",
    "- AI 的理解：“有一个工具叫查询用户位置，我直接调用它就行，不需要我提供任何参数。”\n",
    "- AI 的行为：当用户问“我在哪？”时，AI 只是简单地输出一个调用指令：Call tool: get_user_location（括号里是空的）。\n",
    "\n",
    "\n",
    "代码执行的真相（Runtime）\n",
    "当工具真正开始运行时，LangChain 框架会把后台准备好的 Context 偷偷塞给这个函数。\n",
    "\n",
    "代码逻辑：\n",
    "- AI 没传参数？没关系。\n",
    "- 我看了一眼 runtime.context（后台偷偷塞进来的）。\n",
    "- 哦，原来现在的 user_id 是 \"1\"。\n",
    "- 那我返回 \"Florida\"。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "611d576d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 上下文感知工具\n",
    "# 在生成的工具 Schema 中，runtime 参数会被隐藏。\n",
    "# AI 看到的只是一个名为 get_user_location 的工具，且不需要任何输入参数。\n",
    "@tool\n",
    "def get_user_location(runtime: ToolRuntime[Context]) -> str:\n",
    "    \"\"\"Retrieve user information based on user ID.\"\"\"\n",
    "    user_id = runtime.context.user_id\n",
    "    return \"Florida\" if user_id == \"1\" else \"SF\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40af2d7d",
   "metadata": {},
   "source": [
    "3. 代理响应匹配特定模式，可以选择定义结构化响应格式\n",
    "   LangChain 中非常核心且强大的功能：结构化输出（Structured Output）。之前我们让 AI 说话，它吐出来的是一段字符串（String）；现在我们用这段代码，要求 AI 吐出来的是一个对象（Object）。\n",
    "\n",
    "   比如：\n",
    "   ```python\n",
    "    ResponseFormat(\n",
    "        punny_response=\"Why did the sun go to school? To get brighter!\",\n",
    "        weather_conditions=\"Sunny\"\n",
    "    )\n",
    "   ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bee6510a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "# We use a dataclass here, but Pydantic models are also supported.\n",
    "# 这个ResponseFormat定义了希望 AI 最终返回的数据格式。会被 LangChain 翻译成 JSON Schema 发给 AI。\n",
    "\n",
    "\n",
    "\n",
    "# punny_response: 这里其实实际上还是在写提示词\n",
    "# weather_conditions: 任何有趣的天气信息（如果有的话）\n",
    "\n",
    "# 如果 AI 刚才查了天气（调用了工具），它就会把天气填在这里。\n",
    "# 如果用户只是问“讲个笑话”，AI 没查天气，它就会明智地把这里设为 None\n",
    "\n",
    "@dataclass\n",
    "class ResponseFormat:\n",
    "    \"\"\"Response schema for the agent.\"\"\"\n",
    "    # A punny response (always required)\n",
    "    punny_response: str\n",
    "    # Any interesting information about the weather if available\n",
    "    weather_conditions: str | None = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44cd6f8",
   "metadata": {},
   "source": [
    "4. 添加一个记忆，允许代理记住之前的对话和上下文"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "14108390",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "\n",
    "checkpointer = InMemorySaver()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35bc171f",
   "metadata": {},
   "source": [
    "    AI 拥有了一个持久化存储的能力，在这里是用内存进行存储\n",
    "\n",
    "\n",
    "    注意： 这是为了方便演示才使用的`InemorySaver`。如果应用上线使用，需要换成 PostgresSaver 或 SqliteSaver，把记忆存到硬盘数据库里，这样服务器重启后用户的数据还在。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d45fee0",
   "metadata": {},
   "source": [
    "5. 进行组装并运行这个agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6a64b87a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ResponseFormat</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">punny_response</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"Looks like Florida is still basking in its 'sun-sational' glow! The forecast remains as bright </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">as ever - you could say the weather is absolutely 'ray-diant'! Don't forget your sunscreen, because this forecast </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">is 'sun-sational'!\"</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">weather_conditions</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"It's always sunny in Florida!\"</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mResponseFormat\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mpunny_response\u001b[0m=\u001b[32m\"Looks\u001b[0m\u001b[32m like Florida is still basking in its 'sun-sational' glow! The forecast remains as bright \u001b[0m\n",
       "\u001b[32mas ever - you could say the weather is absolutely 'ray-diant'! Don't forget your sunscreen, because this forecast \u001b[0m\n",
       "\u001b[32mis 'sun-sational'!\"\u001b[0m,\n",
       "    \u001b[33mweather_conditions\u001b[0m=\u001b[32m\"It\u001b[0m\u001b[32m's always sunny in Florida!\"\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ResponseFormat</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">punny_response</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"You're 'weather' welcome! I'm always 'precipitation' to help with your weather needs. If you </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">need another forecast, just 'rain' me a question - I'll be 'cirrus' to assist!\"</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">weather_conditions</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mResponseFormat\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mpunny_response\u001b[0m=\u001b[32m\"You\u001b[0m\u001b[32m're 'weather' welcome! I'm always 'precipitation' to help with your weather needs. If you \u001b[0m\n",
       "\u001b[32mneed another forecast, just 'rain' me a question - I'll be 'cirrus' to assist!\"\u001b[0m,\n",
       "    \u001b[33mweather_conditions\u001b[0m=\u001b[3;35mNone\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableConfig # 导入类型\n",
    "\n",
    "\n",
    "agent = create_agent(\n",
    "    model= MiniMax_M2_chat,\n",
    "    system_prompt=SYSTEM_PROMPT,\n",
    "    tools=[get_user_location, get_weather_for_location],\n",
    "    context_schema=Context,\n",
    "    response_format=ResponseFormat,\n",
    "    checkpointer=checkpointer\n",
    ")\n",
    "\n",
    "# `thread_id` is a unique identifier for a given conversation.\n",
    "config: RunnableConfig = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "response = agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather outside?\"}]},\n",
    "    config=config,\n",
    "    context=Context(user_id=\"1\")\n",
    ")\n",
    "rprint(response['structured_response'])\n",
    "# print(response['structured_response'])\n",
    "\n",
    "\n",
    "# ResponseFormat(\n",
    "#     punny_response=\"Florida is still having a 'sun-derful' day! The sunshine is playing 'ray-dio' hits all day long! I'd say it's the perfect weather for some 'solar-bration'! If you were hoping for rain, I'm afraid that idea is all 'washed up' - the forecast remains 'clear-ly' brilliant!\",\n",
    "#     weather_conditions=\"It's always sunny in Florida!\"\n",
    "# )\n",
    "\n",
    "\n",
    "# Note that we can continue the conversation using the same `thread_id`.\n",
    "response = agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"thank you!\"}]},\n",
    "    config=config,\n",
    "    context=Context(user_id=\"1\")\n",
    ")\n",
    "\n",
    "rprint(response['structured_response'])\n",
    "# ResponseFormat(\n",
    "#     punny_response=\"You're 'thund-erfully' welcome! It's always a 'breeze' to help you stay 'current' with the weather. I'm just 'cloud'-ing around waiting to 'shower' you with more forecasts whenever you need them. Have a 'sun-sational' day in the Florida sunshine!\",\n",
    "#     weather_conditions=None\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db16e13",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-academy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
